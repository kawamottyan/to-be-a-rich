{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc1301c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Using cached webdriver_manager-3.8.5-py2.py3-none-any.whl (27 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     --------------- ---------------------- 30.7/77.1 kB 660.6 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 77.1/77.1 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from webdriver_manager) (2.28.2)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from webdriver_manager) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from requests->webdriver_manager) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from requests->webdriver_manager) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from requests->webdriver_manager) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\kawam\\miniforge3\\envs\\keiba\\lib\\site-packages (from tqdm->webdriver_manager) (0.4.6)\n",
      "Installing collected packages: tqdm, python-dotenv, webdriver_manager\n",
      "Successfully installed python-dotenv-1.0.0 tqdm-4.65.0 webdriver_manager-3.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495f1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#スクレイピング\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select,WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "770b0ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|█████████████████████████████████████████████████████████| 6.79M/6.79M [00:09<00:00, 714kB/s]\n",
      "C:\\Users\\kawam\\AppData\\Local\\Temp\\ipykernel_22540\\414228972.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function selenium.webdriver.support.expected_conditions.presence_of_all_elements_located.<locals>._predicate(driver)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "wait = WebDriverWait(driver,10)\n",
    "\n",
    "URL = \"https://db.netkeiba.com/?pid=race_search_detail\"\n",
    "driver.get(URL)\n",
    "time.sleep(1)\n",
    "wait.until(EC.presence_of_all_elements_located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29cc9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 月ごとに検索\n",
    "year = 2022\n",
    "month = 11\n",
    "\n",
    "# 期間を選択\n",
    "year_element =driver.find_element(By.NAME,\"start_year\")\n",
    "year_select = Select(year_element)\n",
    "year_select.select_by_value(str(year))\n",
    "month_element = driver.find_element(By.NAME,\"start_mon\")\n",
    "month_select = Select(month_element)\n",
    "month_select.select_by_value(str(month))\n",
    "end_year_element = driver.find_element(By.NAME,\"end_year\")\n",
    "end_year_select = Select(end_year_element)\n",
    "end_year_select.select_by_value(str(year))\n",
    "end_mon_element = driver.find_element(By.NAME,\"end_mon\")\n",
    "end_mon_select = Select(end_mon_element)\n",
    "end_mon_select.select_by_value(str(month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b894ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#中央競馬場をチェック\n",
    "for i in range(1,4):\n",
    "    terms = driver.find_element(By.ID,\"check_Jyo_\"+ str(i).zfill(2))\n",
    "    terms.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da03a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表示件数を選択(20,50,100の中から最大の100へ)\n",
    "list_element = driver.find_element(By.NAME,'list')\n",
    "list_select = Select(list_element)\n",
    "list_select.select_by_value(\"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォームを送信\n",
    "frm = driver.find_element(By.CSS_SELECTOR,\"#db_search_detail_form > form\")\n",
    "frm.submit()\n",
    "time.sleep(5)\n",
    "wait.until(EC.presence_of_all_elements_located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('text'):\n",
    "    os.makedirs('text')\n",
    "\n",
    "with open(\"text/\"+str(year)+\"-\"+str(month)+\".txt\", mode='w') as f:\n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        wait.until(EC.presence_of_all_elements_located)\n",
    "        all_rows = driver.find_element(By.CLASS_NAME,'race_table_01').find_elements(By.TAG_NAME,\"tr\")\n",
    "        for row in range(1, len(all_rows)):\n",
    "            race_href=all_rows[row].find_elements(By.TAG_NAME,\"td\")[4].find_element(By.TAG_NAME,\"a\").get_attribute(\"href\")\n",
    "            f.write(race_href+\"\\n\")\n",
    "        try:\n",
    "            target = driver.find_elements(By.LINK_TEXT,\"次\")[0]\n",
    "            driver.execute_script(\"arguments[0].click();\", target) \n",
    "        except IndexError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf27301",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"html\"+\"/\"+str(year)+\"/\"+str(month)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "        \n",
    "with open(\"text/\"+str(year)+\"-\"+str(month)+\".txt\", \"r\") as f:\n",
    "    urls = f.read().splitlines()\n",
    "    for url in urls:\n",
    "        list = url.split(\"/\")\n",
    "        race_id = list[-2]\n",
    "        save_file_path = save_dir+\"/\"+race_id+'.html'\n",
    "        response = requests.get(url)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        html = response.text\n",
    "        time.sleep(5)\n",
    "        with open(save_file_path, 'w') as file:\n",
    "            file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data_columns=[\n",
    "    'race_id',\n",
    "    'race_round',\n",
    "    'race_title',\n",
    "    'race_course',\n",
    "    'weather',\n",
    "    'ground_status',\n",
    "    'time',\n",
    "    'date',\n",
    "    'where_racecourse',\n",
    "    'total_horse_number',\n",
    "    'frame_number_first',\n",
    "    'horse_number_first',\n",
    "    'frame_number_second',\n",
    "    'horse_number_second',\n",
    "    'frame_number_third',\n",
    "    'horse_number_third',\n",
    "    'tansyo',\n",
    "    'hukusyo_first',\n",
    "    'hukusyo_second',\n",
    "    'hukusyo_third',\n",
    "    'wakuren',\n",
    "    'umaren',\n",
    "    'wide_1_2',\n",
    "    'wide_1_3',\n",
    "    'wide_2_3',\n",
    "    'umatan',\n",
    "    'renhuku3',\n",
    "    'rentan3'\n",
    "    ]\n",
    "\n",
    "horse_data_columns=[\n",
    "    'race_id',\n",
    "    'rank',\n",
    "    'frame_number',\n",
    "    'horse_number',\n",
    "    'horse_id',\n",
    "    'sex_and_age',\n",
    "    'burden_weight',\n",
    "    'rider_id',\n",
    "    'goal_time',\n",
    "    'goal_time_dif',\n",
    "    'time_value',\n",
    "    'half_way_rank',\n",
    "    'last_time',\n",
    "    'odds',\n",
    "    'popular',\n",
    "    'horse_weight',\n",
    "    'tame_time',\n",
    "    'tamer_id',\n",
    "    'owner_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e28a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rade_and_horse_data_by_html(race_id, html):\n",
    "    race_list = [race_id]\n",
    "    horse_list_list = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # race基本情報\n",
    "    data_intro = soup.find(\"div\", class_=\"data_intro\")\n",
    "    race_list.append(data_intro.find(\"dt\").get_text().strip(\"\\n\")) # race_round\n",
    "    race_list.append(data_intro.find(\"h1\").get_text().strip(\"\\n\")) # race_title\n",
    "    race_details1 = data_intro.find(\"p\").get_text().strip(\"\\n\").split(\"\\xa0/\\xa0\")\n",
    "    race_list.append(race_details1[0]) # race_course\n",
    "    race_list.append(race_details1[1]) # weather\n",
    "    race_list.append(race_details1[2]) # ground_status\n",
    "    race_list.append(race_details1[3]) # time\n",
    "    race_details2 = data_intro.find(\"p\", class_=\"smalltxt\").get_text().strip(\"\\n\").split(\" \")\n",
    "    race_list.append(race_details2[0]) # date\n",
    "    race_list.append(race_details2[1]) # where_racecourse\n",
    "\n",
    "\n",
    "    result_rows = soup.find(\"table\", class_=\"race_table_01 nk_tb_common\").findAll('tr') # レース結果\n",
    "    # 上位3着の情報\n",
    "    race_list.append(len(result_rows)-1) # total_horse_number\n",
    "    for i in range(1,4):\n",
    "        row = result_rows[i].findAll('td')\n",
    "        race_list.append(row[1].get_text()) # frame_number_first or second or third\n",
    "        race_list.append(row[2].get_text()) # horse_number_first or second or third\n",
    "\n",
    "\n",
    "    # 払い戻し(単勝・複勝・三連複・3連単)\n",
    "    pay_back_tables = soup.findAll(\"table\", class_=\"pay_table_01\")\n",
    "\n",
    "    pay_back1 = pay_back_tables[0].findAll('tr') # 払い戻し1(単勝・複勝)\n",
    "    race_list.append(pay_back1[0].find(\"td\", class_=\"txt_r\").get_text()) #tansyo\n",
    "    hukuren = pay_back1[1].find(\"td\", class_=\"txt_r\")\n",
    "    tmp = []\n",
    "    for string in hukuren.strings:\n",
    "        tmp.append(string)\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            race_list.append(tmp[i]) # hukuren_first or second or third\n",
    "        except IndexError:\n",
    "            race_list.append(\"0\")\n",
    "\n",
    "    # 枠連\n",
    "    try:\n",
    "        race_list.append(pay_back1[2].find(\"td\", class_=\"txt_r\").get_text())\n",
    "    except IndexError:\n",
    "        race_list.append(\"0\")\n",
    "\n",
    "    # 馬連\n",
    "    try:\n",
    "        race_list.append(pay_back1[3].find(\"td\", class_=\"txt_r\").get_text())\n",
    "    except IndexError:\n",
    "        race_list.append(\"0\")\n",
    "\n",
    "\n",
    "\n",
    "    pay_back2 = pay_back_tables[1].findAll('tr') # 払い戻し2(三連複・3連単)\n",
    "\n",
    "    # wide 1&2\n",
    "    wide = pay_back2[0].find(\"td\", class_=\"txt_r\")\n",
    "    tmp = []\n",
    "    for string in wide.strings:\n",
    "        tmp.append(string)\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            race_list.append(tmp[i]) # hukuren_first or second or third\n",
    "        except IndexError:\n",
    "            race_list.append(\"0\")\n",
    "\n",
    "    # umatan\n",
    "    race_list.append(pay_back2[1].find(\"td\", class_=\"txt_r\").get_text()) #umatan\n",
    "\n",
    "    race_list.append(pay_back2[2].find(\"td\", class_=\"txt_r\").get_text()) #renhuku3\n",
    "    try:\n",
    "        race_list.append(pay_back2[3].find(\"td\", class_=\"txt_r\").get_text()) #rentan3\n",
    "    except IndexError:\n",
    "        race_list.append(\"0\")\n",
    "\n",
    "    # horse data\n",
    "    for rank in range(1, len(result_rows)):\n",
    "        horse_list = [race_id]\n",
    "        result_row = result_rows[rank].findAll(\"td\")\n",
    "        # rank\n",
    "        horse_list.append(result_row[0].get_text())\n",
    "        # frame_number\n",
    "        horse_list.append(result_row[1].get_text())\n",
    "        # horse_number\n",
    "        horse_list.append(result_row[2].get_text())\n",
    "        # horse_id\n",
    "        horse_list.append(result_row[3].find('a').get('href').split(\"/\")[-2])\n",
    "        # sex_and_age\n",
    "        horse_list.append(result_row[4].get_text())\n",
    "        # burden_weight\n",
    "        horse_list.append(result_row[5].get_text())\n",
    "        # rider_id\n",
    "        horse_list.append(result_row[6].find('a').get('href').split(\"/\")[-2])\n",
    "        # goal_time\n",
    "        horse_list.append(result_row[7].get_text())\n",
    "        # goal_time_dif\n",
    "        horse_list.append(result_row[8].get_text())\n",
    "        # time_value(premium)\n",
    "        horse_list.append(result_row[9].get_text())\n",
    "        # half_way_rank\n",
    "        horse_list.append(result_row[10].get_text())\n",
    "        # last_time(上り)\n",
    "        horse_list.append(result_row[11].get_text())\n",
    "        # odds\n",
    "        horse_list.append(result_row[12].get_text())\n",
    "        # popular\n",
    "        horse_list.append(result_row[13].get_text())\n",
    "        # horse_weight\n",
    "        horse_list.append(result_row[14].get_text())\n",
    "        # tame_time(premium)\n",
    "        horse_list.append(result_row[15].get_text())\n",
    "        # 16:コメント、17:備考\n",
    "        # tamer_id\n",
    "        horse_list.append(result_row[18].find('a').get('href').split(\"/\")[-2])\n",
    "        # owner_id\n",
    "        horse_list.append(result_row[19].find('a').get('href').split(\"/\")[-2])\n",
    "\n",
    "        horse_list_list.append(horse_list)\n",
    "\n",
    "    return race_list, horse_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DIR = \"csv\"\n",
    "if not os.path.isdir(CSV_DIR):\n",
    "    os.makedirs(CSV_DIR)\n",
    "save_race_csv = CSV_DIR+\"/race-\"+str(year)+\"-\"+str(month)+\".csv\"\n",
    "horse_race_csv = CSV_DIR+\"/horse-\"+str(year)+\"-\"+str(month)+\".csv\"\n",
    "\n",
    "race_df = pd.DataFrame(columns=race_data_columns)\n",
    "horse_df = pd.DataFrame(columns=horse_data_columns)\n",
    "\n",
    "html_dir = \"html\"+\"/\"+str(year)+\"/\"+str(month)\n",
    "if os.path.isdir(html_dir):\n",
    "    file_list = os.listdir(html_dir)\n",
    "    for file_name in file_list:\n",
    "        with open(html_dir+\"/\"+file_name, \"r\") as f:\n",
    "            html = f.read()\n",
    "            list = file_name.split(\".\")\n",
    "            race_id = list[-2]\n",
    "            race_list, horse_list_list = get_rade_and_horse_data_by_html(race_id, html) \n",
    "            for horse_list in horse_list_list:\n",
    "                horse_se = pd.Series( horse_list, index=horse_df.columns)\n",
    "                horse_df = pd.concat([horse_df, horse_se.to_frame().T], ignore_index=True)\n",
    "            race_se = pd.Series(race_list, index=race_df.columns )\n",
    "            race_df = pd.concat([race_df, race_se.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df.to_csv(save_race_csv, header=True, index=False)\n",
    "horse_df.to_csv(horse_race_csv, header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
